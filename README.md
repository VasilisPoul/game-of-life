# Εθνικό και Καποδιστριακό Πανεπιστήμιο Αθηνών
### Τμήμα Πληροφορικής και Τηλεπικοινωνιών
### ΘΠ04 - Παράλληλα Συστήματα
### **Εργασία - Game of Life**

**Μέλη**:
 - Βασίλειος Πουλόπουλος - 1115201600141
 - Κωνσταντίνος Χατζόπουλος - 1115201300202

## Εισαγωγή

Για την υλοποίηση της εργασίας, δημιουργήσαμε αρχεία εισόδου για τις μετρήσεις. Για κάθε τμήμα της εργασίας δημιουργήσαμε ξεχωριστο φάκελο στον οποίο υπάρχει ο κώδικας, οι μετρήσεις, τα αποτελέσματα του profiling, τα input files και κάποια bash scripts που υλοποιήσαμε για να αυτοματοποιήσουμε τις παρακάτω διαδικασίες:
 - μεταγλώτηση
 - εκτέλεση στην argo
 - συλλογή μετρήσεων
 - υπολογισμό speedup   
 - υπολογισμό efficiency.
 
Τα scripts αρχικά μεταγλωτίζουν το πρόγραμμα με τις αντίστοιχες παραμέτρους κάθε φορά (μέγεθος προβλήματος **Ν**, αριθμό **διεργασιών** / **nodes** / **cpus** / **threads** / **gpus**) και στη συνέχεια για να το τρέξουν το τοποθετούν στην ουρά με την εντολή **qsub**.

Για όσο βρίσκεται στην ουρά και εκτελείται ή περιμένει να εκτελεστεί, το script ελέγχει σταδιακά με την εντολή qstat αν ολοκληρώθηκε το **job** και αμέσως μετά, με την εντολή append του bash (**>>**) δημιουργεί (αν δεν υπάρχουν ήδη) αρχεία που περιέχουν μόνο τους χρόνους με όνομα **times.txt**. Τέλος για τα script που αφορούν το **MPI** ή **MPI + OpenMp**, το script διαβάζει τα αντίστοιχα αρχεία χρόνων και δημιουργεί τα **speedup.txt** και **efficiency.txt**.

Επίσης, υλοποιήσαμε script που δημιουργεί **επίπεδα αρχεία εισόδου** (όλος ο δισδιάστατος πίνακας χαρακτήρων σε μια γραμμή) τα οποία είναι ίδια με τα **παραγόμενα αρχεία κάθε βήματος**. Επιπλέον για να ελέγξουμε αν κάθε βήμα έχει υπολογιστεί σωστά υλοποιήσαμε script που μετατρέπει τα επιπεδα αρχεία **εισόδου** / **εξόδου** σε δισδιάστατη αναπαράσταση μαύρων ή άσπρων κουτιών (μαύρο κουτί αν η τιμή είναι 0 και άσπρο αν είναι 1).

Στο **MPI** και **MPI + OpenMp** χρησιμοποιούμε command line arguments για να πάρουμε τα εξής:
-   path για το input file
-   path για το output file
-   αριθμό γραμμών
-   αριθμό στηλών

## MPI

Η επιλογή των κόμβων και των διεργασιών έγινε με το σκεπτικό να γεμίζει ο κάθε κόμβος πριν χρησιμοποιήσουμε κάποιο καινούργιο. Αυτό το κάνουμε για να έχουμε όσο το δυνατό λιγότερη επικοινωνία ανάμεσα στους κόμβους ώστε να αποφεύγονται άσκοπες καθυστερήσεις. Δοκιμάζοντας και τις 2 προσεγγίσεις (**πληρότητα κόμβων**, **διασπορά διεργασιών σε κόμβους**) καταλήξαμε στην πρώτη διότι παρατηρήσαμε ότι ήταν πιο γρήγορη, πράγμα το οποίο είναι λογικό αφού στην αρχιτεκτονική κατανεμημένης μνήμης ο κάθε κόμβος δεν έχει πρόσβαση στη μνήμη άλλου κόμβου. Αυτό έχει ως αποτέλεσμα όσο τους αυξάνουμε να αυξάνεται και η επικοινωνία μεταξύ των κόμβων μέσω μηνυμάτων και συνεπως να αυξάνεται η καθυστέρηση.

### Σχεδιασμός

Ο δισδιάστατος πίνακας εισόδου θεωρείται **περιοδικός**, π.χ. αν είμαστε στην τελευταία γραμμή του δισδιάστατου και θέλουμε να πάμε στην ακριβώς από κάτω του, τότε αυτή θα είναι η πρώτη γραμμή κ.ο.κ.

Για την καλύτερη επίτευξη επικοινωνίας μεταξύ των κόμβων, δημιουργούμε μια τοπολογία όπου περιέχει ένα δισδιάστατο πλέγμα από blocks / διεργασίες διαστάσεων **sqrt( # of processes ) * sqrt( # of processes )**. Με αυτόν τον τρόπο, κάθε διεργασία είναι στοιχισμένη σε κάποια θέση στο δισδιάστατο πλέγμα και επεξεργάζεται ένα μόνο block / υποπίνακα του συνολικού προβλήματος.

### Σχεδιασμός MPI κώδικα
Για να μπορέσει να τρέξει το MPI πρόγραμμα, πρέπει να γίνει αρχικοποίηση. Για να γινει αυτό καλούμε τη συνάρτηση **MPI_Init()**. Αμέσως μετά δημιουργούμε την τοπολογία μέσω της συνάρτησης **setupGrid()** όπου αρχικοποιεί τη δομή **gridInfo** με τα δεδομένα της αντίστοιχης διεργασίας.

**struct gridInfo:**
```c
typedef struct gridInfo {  
  MPI_Comm gridComm;		// communicator for entire grid  
  Neighbors neighbors;		// neighbor processes  
  int processes;			// total number of processes  
  int gridRank;				// rank of current process in gridComm  
  int gridDims[2];			// grid dimensions  
  int gridCoords[2];		// grid coordinates  
  int blockDims[2]; 		// block dimensions  
  int localBlockDims[2];	// local block dimensions  
  int stepGlobalChanges;  
  int stepLocalChanges;  
} GridInfo;
```
Τα δεδομένα που αρχικοποιεί η συνάρτηση **setupGrid()**  σε κάθε διεργασία είναι:
 - ο communicator που ανήκει στο πλέγμα,   οι γειτονικές διεργασίες,   
 - ο συνολικός αριθμός διεργασιών
 - η τάξη της διεργασίας στο πλέγμα,   
 - τις διαστάσεις του πλέγματος
 - τις συντεταγμένες της στο πλέγμα  καθώς και
 - τις διαστάσεις του συνολικού πίνακα αλλά και του
 - τοπικού (δικού της) πίνακα.

Για να δημιουργηθεί η τοπολογία καλούμε την συνάρτηση **MPI_Cart_create()** και για να λάβουμε τις συντεταγμένες κάθε διεργασίας την **MPI_Cart_coords()**.

Για τον υπολογισμό κάθε βήματος χρειάζεται να ξέρουμε την προηγούμενη κατάσταση του παιχνιδιού για  να υπολογίσουμε την τρέχουσα. Για το λόγο αυτό δεσμέυουμε δυναμικά με χρήση της συνάρτησης **malloc()** 2 δισδιάστατους πίνακες χαρακτήρων (**old**, **current**) διαστάσεων `grid.localBlockDims[0] + 2, grid.localBlockDims[1] + 2` δηλαδή τις διαστάσεις που αντιστοιχούν στον κάθε υποπίνακα του συνολικού προβλήματος **αυξημένο κατα 2*2 διαστάσεις** για να συμπεριληφθούν και οι γειτονικές τιμές.

#### MPI Datatypes

Επειδή υπήρχε η ανάγκη να στέλνουμε και να λαμβάνουμε γραμμές και στήλες από τους τοπικούς αυτούς πίνακες σε γειτονικές διεργασίες, δημιουργήσαμε **datatypes** με τις κατάλληλες συναρτήσεις (**MPI_Type_vector()**, **MPI_Type_commit()**) γιατί χρειαζόμασταν ένα τρόπο με τον οποίο ή κάθε διεργασία θα αποθηκεύει στον δικό της πίνακα τις τιμές των πινάκων των γειτονικών block για να μπορει να υπολογίσει τις ακριανές τιμές του πίνακά της.

Το MPI θεωρεί πως για τα datatypes τα δεδομένα του buffer αποστολής / λήψης πρέπει να είναι σε συνεχόμενες θέσεις μνήμης, και τα offset για γραμμές και στήλες να είναι σταθερα, οπότε χρειαζόταν οι δισδιάστατοι πίνακες **old** και **current** να είναι σε συνεχόμενες θέσεις μνήμης. Γι αυτό το λόγο υλοποιήσαμε την συνάρτηση **allocate2DArray()** με τέτοιο τρόπο ώστε να δημιουργεί ένα δισδιάστατο πίνακα σε συνεχόμενες θέσεις μνήμης.

#### Είσοδος δεδομένων αρχικής κατάστασης παιχνιδιού

Αν υπάρχει αρχείο εισόδου χρησιμοποιούμε ένα **subarray** datatype που δημιουργήσαμε για την ανάγνωση των σωστών περιοχών/τμημάτων του αρχείου εισόδου από το αντίστοιχο process μέσω της **MPI_File_set_view()** και το διαβάζουμε μέσω της **MPI_File_read()**. Στην περίπτωση που δεν υπάρχει αρχείο εισόδου, δημιουργείται ένας τυχαίος πίνακας με ολόκληρη την αρχική κατάσταση του παιχνιδιού μέσω της συνάρτησης **initialize_block()** και με τη βοήθεια της συνάρτησης int **scatter2DArray()** (*κώδικας από το βιβλίο MPI ΘΕΩΡΙΑ ΚΑΙ ΕΦΑΡΜΟΓΕΣ*), ο πίνακας διασπείρεται σε όλες τις διεργασίες του δίσδιάστατου πλέγματος. Με αυτόν τον τρόπο η κάθε διεργασία έχει στον τοπικό της υποπίνακα '**old**' το αντίστοιχο υποπρόβλημα που καλείται να υπολογίσει.

#### Επικοινωνία μεταξύ των διεργασιών του πλέγματος (Κεντρική επανάλληψη)

Για την επίτευξη της αποστολής των 8 μηνυμάτων (ένα για κάθε γείτονα) στις γειτονικές διεργασίες καθώς και τη λήψη άλλων 8, χωρίς την επιβάρυνση της κεντρικής επανάληψης με επιπλέον υπολογισμούς (offsets, κτλ), απαιτείται η αρχικοποίηση τους μέσω της συνάρτησης **MPI_Send_init()** ώστε να δημιουργηθούν 16 requests τα οποία θα είναι έτοιμα προς εκκίνηση όταν αυτό χρειαστεί με τη χρήση της συνάρτησης **MPI_Startall()**.  Στη συνέχεια γίνονται οι υπολογισμοί των εσωτερικών κυττάρων και κατόπιν, καλείται η **MPI_Waitall()** ωστε πριν υπολογιστούν και τα εξωτερικά κύτταρα να έχουν σταλθεί όλα τα γειτονικά σε κάθε block για να γίνουν σωστά οι υπολογισμοί. 

#### Υπολογισμοί

Η συνάρτηση **calculate()** που έχουμε υλοποιήσει για τους υπολογισμούς είναι **inline** για να μην υπάρχουν καθυστερήσεις, επιπλέον, η συνάρτηση αυτή δέχεται ώς όρισμα ένα δείκτη στην ακέραια μεταβλητή **changes** όπου αν υπάρξει αλλαγή στο συγκεκριμένο κύτταρο που εξετάζεται εκείνη τη στιγμή, η μεταβλητή αυτή αυξάνεται κατά 1. 

#### Swap

Στο τέλος της επανάληψης, το αποτέλεσμα που υπολογίστηκε από τον πίνακα '**old'**, θα αποθηκευτεί στον πινακα '**current**'. Έτσι, στην επόμενη επανάληψη θα πρέπει η παλιά κατάσταση του παιχνιδιού να αντικατασταθεί με την current του προηγούμενου βήματος. Για το λόγο αυτό, πρέπει να γίνει **swap** του old με τον current.

### Μετρήσεις χρόνων εκτέλεσης (Χωρίς reduce)

| |1 Process| 4 Processes| 16 Processes| 64 Processes| 
|--|--|--|--|--| 
| **320x320**     | 0.745   |  **0.216** | 0.202     | 0.292      |
| **640x640**     | 3.354   |  0.770     | **0.313** | 0.310      |
| **1280x1280**   | 13.268  |  3.383     | 0.851     | **0.391**  |
| **2560x2560**   | 53.610  |  13.459    | 3.466     | **0.917**  |
| **5120x5120**   | 213.743 |  53.943    | 18.438    | **8.652**  |
| **10240x10240** | -       |  215.087   | 54.659    | **13.750** |
| **20480x20480** | -       |  -         | 217.871   | **57.988** |
| **40960x40960** | -       |  -         | -         | **218.213**|

Παρατηρούμε ότι όσο αυξάνεται το μέγεθος του προβλήματος, μεγαλώνοντας τον αριθμό των διεργασιών ότι στα πράσινα υπάρχει καλύτερη κλιμάκωση, συνεχίζοντας όμως να μεγαλώνουμε τον αριθμό, βλέπουμε ότι δεν συμφέρει (κίτρινα) διότι γίνεται σπατάλη πόρων για την επίτευξη του ίδιου περίπου χρόνου με τα πράσινα.

Επιπλέον, για μεγέθη μεγαλύτερα από 5120x5120 δεν έφτασε το όριο των 10 λεπτών και σταμάτησε η εκτέλεση.


### Yπολογισμός speedup

| |1 Process| 4 Processes| 16 Processes| 64 Processes| 
|--|--|--|--|--| 
| **320x320**     | 1.0  |  **3.449** | 3.681      | 2.542       |
| **640x640**     | 1.0  |  4.357     | **10.705** | 10.815      |
| **1280x1280**   | 1.0  |  3.921     | 15.600     | **33.949**  |
| **2560x2560**   | 1.0  |  3.983     | 15.468     | **58.478**  |
| **5120x5120**   | 1.0  |  3.962     | 11.593     | **24.704**  |

Όπως αναφέραμε και παραπάνω, δεν υπάρχει λόγος να αυξήσουμε τις διεργασίες για τις περιπτώσεις που είναι με κίτρινο.

### Yπολογισμός efficiency

| |1 Process| 4 Processes| 16 Processes| 64 Processes| 
|--|--|--|--|--| 
| **320x320**     | 1.0 | **1.724** | 0.920    | 0.318      |
| **640x640**     | 1.0 | 2.178     |**2.676** | 1.352      |
| **1280x1280**   | 1.0 | 1.961     | 3.900    | **4.244** |
| **2560x2560**   | 1.0 | 1.992     | 3.867    | **7.310** |
| **5120x5120**   | 1.0 | 1.981     | 2.898    | **3.088**  |

Με τη βοήθεια του τύπου υπολογισμού του efficiency (speedup / processes), επιβεβαιώσαμε τον παραπάνω συλλογισμό μας.

### Μετρήσεις χρόνων εκτέλεσης (Με reduce)

| |1 Process| 4 Processes| 16 Processes| 64 Processes| 
|--|--|--|--|--| 
| **320x320**     | 0.747    |  **0.216**  | 0.217      | 0.330      |
| **640x640**     | 3.364    |  0.774      | **0.333**  | 0.348      |
| **1280x1280**   | 13.315   |  3.384      | **0.879**  | 0.443      |
| **2560x2560**   | 53.737   |  13.470     | 3.502      | **0.977**  |
| **5120x5120**   | 214.299  |  54.121     | 13.764     | **3.594**  |

### Yπολογισμός speedup

| |1 Process| 4 Processes| 16 Processes| 64 Processes| 
|--|--|--|--|--| 
| **320x320**     | 1.0  | **1.729** | 0.859     | 0.283      |
| **640x640**     | 1.0  | 2.172     | **2.525** | 1.208      |
| **1280x1280**   | 1.0  | 1.968     | **3.786** | 3.757      |
| **2560x2560**   | 1.0  | 1.995     | 3.836     | **6.874**  |
| **5120x5120**   | 1.0  | 1.980     | 3.892     | **7.454**  |

### Yπολογισμός efficiency

| |1 Process| 4 Processes| 16 Processes| 64 Processes| 
|--|--|--|--|--| 
| **320x320**     | 1.0 | **1.724** | 0.920    | 0.318      |
| **640x640**     | 1.0 | 2.178     |**2.676** | 1.352      |
| **1280x1280**   | 1.0 | 1.961     | 3.900    | **4.244** |
| **2560x2560**   | 1.0 | 1.992     | 3.867    | **7.310** |
| **5120x5120**   | 1.0 | 1.981     | 2.898    | **3.088**  |

Στο πρόγραμμά μας δεν παρατηρούμε κάποια σημαντική διαφορά στους χρόνους. Παρόλα αυτά είναι ένα παραπάνω κόστος που γίνεται σε κάθε επανάληψη. Όσο αυξάνονται οι διεργασίες, αυξάνονται και τα μηνύματα λόγο επικοινωνίας στο σερβερ οπότε αυξάνεται και η καθυστερηση.

## MPI+OPENMP



## CUDA

Για την υλοποίηση με cuda, αρχικά έπρεπε να σκεφτούμε πως θα χωρίσουμε σε κάθε thread το πρόβλημα μιας και η GPU έχει χιλιάδες νήματα, έτσι αποφασίσαμε να χρησιμοποιήσουμε ένα δισδιάστατο πλέγμα από blocks των δύο διαστάσεων κάνοντας την παραδοχή πως κάθε κύτταρο θα το επεξεργάζεται και ένα διαφορετικό νήμα.

### Σχεδιασμός host κώδικα

Κατά την εκκίνηση του προγράμματος είναι απαραίτητο να γίνουν κάποιες αρχικοποιήσεις στον host κώδικα ώστε να γίνει κλήση της kernel συνάρτησης (device). Πιο συγκεκριμένα, γίνεται αρχικοποίηση των μεταβλητών m, n τύπου dim3 όπου αποθηκεύονται οι διαστάσεις των μπλοκς (m) καθώς και οι διαστάσεις του πλέγματος από μπλοκ (n). Μετά την παραπάνω αρχικοποίηση, πραγματοποιείται έλεγχος assert ώστε να εξασφαλιστεί πως το πρόγραμμα θα εκτελεστεί με τις σωστές διαστάσεις πλέγματος και νημάτων ανα μπλοκ.

Όταν περάσει ο έλεγχος, δεσμεύεται στην host memory δυναμικά ένας δισδιάστατος πίνακας χαρακτήρων (σε συνεχόμενες θέσεις μνήμης) ο οποίος θα αρχικοποιηθεί αργότερα από το αρχείο εισόδου με τα δεδομένα του πρώτου βήματος με τελικό σκοπό να αντιγραφεί στην device memory (device_old). Για να την επίτευξη της αντιγραφής αυτής, είναι απαραίτητο να γίνει δέσμευση N * N * sizeof(char) bytes στην device memory μέσω της συνάρτησης cudaMalloc() και αμέσως μετά να κληθεί η cudaMemcpy().

Στην κεντρική επανάληψη, καλείται η συνάρτηση kernel που αναλαμβάνει να υπολογίσει ολόκληρο το βήμα χωρίζοντας το πρόβλημα σε blocks και νήματα.

Κατά την ολοκλήρωση υπολογισμού του τρέχοντος βήματος μέσα στην κεντρική επανάληψη, είναι απαραίτητο να αρχικοποιηθεί ο πίνακας device_old με ‘0’ καθώς και να γίνει αντικατάσταση του device_old με τα δεδομένα του device_current πίνακα για την αρχικοποίηση του επόμενου βήματος.

### Σχεδιασμός device κώδικα (kernel)

Αρχικά θεωρούμε πως ο **τετραγωνικός πίνακας μεγέθους N x N** αντιγράφεται στην μνήμη της κάρτας γραφικών και πως κάθε μπλοκ από νήματα θα έχει στη διάθεσή του και θα επεξεργάζεται μόνο ένα μέρος αυτού. Δεδομένου ότι κάθε block είναι δυνατό να έχει την δική του κοινή ιδιωτική μνήμη, αρχικά δεσμεύουμε αυτή τη μνήμη με την εντολή `__shared__ char local[M + 2][M + 2];` δημιουργώντας έτσι για κάθε ένα από αυτά έναν πίνακα διαστάσεων `(Μ+2) *( Μ+2)` ώστε να αποθηκευτούν περιμετρικά και οι τιμές των κυττάρων από τα γειτονικά μπλοκ (πάνω, κάτω, αριστερά, δεξιά, διαγώνιες). Έπειτα, αντιγράφουμε το αντίστοιχο **2d** κομμάτι που αντιστοιχεί στην τοπική μνήμη του κάθε μπλοκ καθώς και τα γειτονικά του. Η αντιγραφή αυτή είναι πολύ σημαντικό να γίνει διότι την ώρα που θα γίνονται οι υπολογισμοί στο εκάστοτε **block**, δεν θα χρειάζεται να γίνεται προσπέλαση της **Global Memory** (η οποία απαιτεί 800 κύκλους) αλλα θα έχει άμεση πρόσβαση στις τιμές των γειτονικών κυττάρων μέσω της τοπικής του μνήμης κάνοντάς το έτσι πιο γρήγορο.

#### Υπολογισμοί

Για τους υπολογισμούς που εκτελεί κάθε νήμα, όταν χρειαστεί να διαβάσει την παλιά κατάσταση ενός γειτονικού κυττάρου για να μπορέσει να ενημερώσει την καινούργια, δεν είναι απαραίτητο να αποκτήσει πρόσβαση στον πίνακα ‘**device_old**’ που είναι στην **global memory**. Αντιθέτως, το μόνο που κάνει είναι να διαβάσει την αντίστοιχη τιμή που θέλει από την τοπική κοινή μνήμη (‘**local**’) του block που ανήκει πολύ πιο γρήγορα από τι θα το έκανε διαβάζοντας την ίδια τιμή από τον πίνακα ‘**device_old**’ (**global memory**). Παρόλα αυτά, για να γραφτεί το αποτέλεσμα του τρέχοντος βηματος, θα πρέπει να γίνει write στον πίνακα ‘**device_current**’ (**global memory**), κάτι το οποίο προσθέτει καθυστέρηση αλλά είναι αναπόφευκτο.

### Μετρήσεις χρόνων εκτέλεσης

| |4 threads / block| 8 threads / block| 16 threads / block|32 threads / block| 64 threads/block |
|--|--|--|--|--|--|
| **320x320**     | 0.06  | 0.05  | 0.06  | 0.07 | **0.04** |
| **640x640**     | 0.19  | 0.15  | 0.18  | 0.19 | **0.09** |
| **1280x1280**   | 0.61  | 0.48  | 0.56  | 0.64 | **0.27** |
| **2560x2560**   | 1.95  | 1.52  | 1.78  | 1.98 | **0.83** |
| **5120x5120**   | 8.08  | 5.98  | 6.84  | 7.60 | **3.15** |

## Συμπερασματα

